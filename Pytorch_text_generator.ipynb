{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "02226526",
   "metadata": {},
   "source": [
    "## Text generation model from scratch\n",
    "- Simple tokenizer \n",
    "- Embedding Layer with PyTorch\n",
    "- Positional encoding\n",
    "- Masking\n",
    "- Attention score\n",
    "- Multi-head Attention for Diverse Perspectives\n",
    "- Decoder Layer\n",
    "- Language model head."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55016636",
   "metadata": {},
   "source": [
    "## Simple tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8779c13b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from typing import List\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "1fb04b6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TokenEmbedding(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    PyTorch module that converts tokens into embeddings.\n",
    "\n",
    "    Input dimension is: (batch_size, sequence_length)\n",
    "    Output dimension is: (batch_size, sequence_length, d_model)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, d_model, number_of_tokens):\n",
    "        super().__init__()\n",
    "        self.embedding_layer = torch.nn.Embedding(\n",
    "            num_embeddings=number_of_tokens,\n",
    "            embedding_dim=d_model\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.embedding_layer(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d371eace",
   "metadata": {},
   "source": [
    "## Positional Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b90e2863",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    Pytorch module that creates a positional encoding matrix. This matrix will later be added to the\n",
    "    transformer's input embeddings to provide a sense of position of the sequence elements.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, d_model, max_sequence_length):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.max_sequence_length = max_sequence_length\n",
    "        self.positional_encoding = self.create_positional_encoding()\n",
    "\n",
    "    def create_positional_encoding(self):\n",
    "        \"\"\"\n",
    "        Creates a positional encoding matrix of size (max_sequence_length, d_model).\n",
    "        \"\"\"\n",
    "\n",
    "        # Initialize positional encoding matrix\n",
    "        positional_encoding = np.zeros((self.max_sequence_length, self.d_model))\n",
    "\n",
    "        # Calculate positional encoding for each position and each dimension\n",
    "        for pos in range(self.max_sequence_length):\n",
    "            for i in range(0, self.d_model, 2):\n",
    "                # Apply sin to even indices in the array; indices in Python start at 0 so i is even.\n",
    "                positional_encoding[pos, i] = np.sin(pos / (10000 ** ((2 * i) / self.d_model)))\n",
    "\n",
    "                if i + 1 < self.d_model:\n",
    "                    # Apply cos to odd indices in the array; we add 1 to i because indices in Python start at 0.\n",
    "                    positional_encoding[pos, i + 1] = np.cos(pos / (10000 ** ((2 * i) / self.d_model)))\n",
    "\n",
    "        # Convert numpy array to PyTorch tensor and return it\n",
    "        return torch.from_numpy(positional_encoding).float().to(get_device())\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Adds the positional encoding to the input embeddings at the corresponding positions.\n",
    "        \"\"\"\n",
    "        # Add positional encodings to input embeddings. The \":\" indexing ensures we only add positional encodings up\n",
    "        # to the length of the sequence in the batch. x.size(0) is the batch size, so this is a way to make sure\n",
    "        # we're not adding extra positional encodings.\n",
    "        return x + self.positional_encoding[:x.size(1), :]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "967a154c",
   "metadata": {},
   "source": [
    "## Self-attention layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "42b73a60",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MaskedSelfAttention(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    Pytorch module for a self attention layer.\n",
    "    This layer is used in the MultiHeadedSelfAttention module.\n",
    "\n",
    "    Input dimension is: (batch_size, sequence_length, embedding_dimension)\n",
    "    Output dimension is: (batch_size, sequence_length, head_dimension)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, embedding_dimension, head_dimension):\n",
    "        super().__init__()\n",
    "        self.embedding_dimension = embedding_dimension\n",
    "        self.head_dimension = head_dimension\n",
    "        self.query_layer = torch.nn.Linear(embedding_dimension, self.head_dimension)\n",
    "        self.key_layer = torch.nn.Linear(embedding_dimension, self.head_dimension)\n",
    "        self.value_layer = torch.nn.Linear(embedding_dimension, self.head_dimension)\n",
    "        self.softmax = torch.nn.Softmax(dim=-1)\n",
    "\n",
    "    def forward(self, x, mask):\n",
    "        \"\"\"\n",
    "        Compute the self attention.\n",
    "\n",
    "        x dimension is: (batch_size, sequence_length, embedding_dimension)\n",
    "        output dimension is: (batch_size, sequence_length, head_dimension)\n",
    "        mask dimension is: (batch_size, sequence_length)\n",
    "\n",
    "        mask values are: 0 or 1. 0 means the token is masked, 1 means the token is not masked.\n",
    "        \"\"\"\n",
    "\n",
    "        # x dimensions are: (batch_size, sequence_length, embedding_dimension)\n",
    "        # query, key, value dimensions are: (batch_size, sequence_length, head_dimension)\n",
    "        query = self.query_layer(x)\n",
    "        key = self.key_layer(x)\n",
    "        value = self.value_layer(x)\n",
    "\n",
    "        # Calculate the attention weights.\n",
    "        # attention_weights dimensions are: (batch_size, sequence_length, sequence_length)\n",
    "        attention_weights = torch.matmul(query, key.transpose(-2, -1))\n",
    "\n",
    "        # Scale the attention weights.\n",
    "        attention_weights = attention_weights / np.sqrt(self.head_dimension)\n",
    "\n",
    "        # Apply the mask to the attention weights, by setting the masked tokens to a very low value.\n",
    "        # This will make the softmax output 0 for these values.\n",
    "        mask = mask.reshape(attention_weights.shape[0], 1, attention_weights.shape[2])\n",
    "        attention_weights = attention_weights.masked_fill(mask == 0, -1e9)\n",
    "\n",
    "        # Softmax makes sure all scores are between 0 and 1 and the sum of scores is 1.\n",
    "        # attention_scores dimensions are: (batch_size, sequence_length, sequence_length)\n",
    "        attention_scores = self.softmax(attention_weights)\n",
    "\n",
    "        # The attention scores are multiplied by the value\n",
    "        # Values of tokens with high attention score get highlighted because they are multiplied by a larger number,\n",
    "        # and tokens with low attention score get drowned out because they are multiplied by a smaller number.\n",
    "        # Output dimensions are: (batch_size, sequence_length, head_dimension)\n",
    "        return torch.bmm(attention_scores, value)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c1f68d86",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Multi-head self-attention layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c633e8e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MaskedMultiHeadedSelfAttention(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    Pytorch module for a multi head attention layer.\n",
    "\n",
    "    Input dimension is: (batch_size, sequence_length, embedding_dimension)\n",
    "    Output dimension is: (batch_size, sequence_length, embedding_dimension)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, embedding_dimension, number_of_heads):\n",
    "        super().__init__()\n",
    "        self.embedding_dimension = embedding_dimension\n",
    "        self.head_dimension = embedding_dimension // number_of_heads\n",
    "        self.number_of_heads = number_of_heads\n",
    "\n",
    "        # Create the self attention modules\n",
    "        self.self_attentions = torch.nn.ModuleList(\n",
    "            [MaskedSelfAttention(embedding_dimension, self.head_dimension) for _ in range(number_of_heads)])\n",
    "\n",
    "        # Create a linear layer to combine the outputs of the self attention modules\n",
    "        self.output_layer = torch.nn.Linear(number_of_heads * self.head_dimension, embedding_dimension)\n",
    "\n",
    "    def forward(self, x, mask):\n",
    "        \"\"\"\n",
    "        Compute the multi head attention.\n",
    "\n",
    "        x dimensions are: (batch_size, sequence_length, embedding_dimension)\n",
    "        mask dimensions are: (batch_size, sequence_length)\n",
    "        mask values are: 0 or 1. 0 means the token is masked, 1 means the token is not masked.\n",
    "        \"\"\"\n",
    "        # Compute the self attention for each head\n",
    "        # self_attention_outputs dimensions are:\n",
    "        # (number_of_heads, batch_size, sequence_length, head_dimension)\n",
    "        self_attention_outputs = [self_attention(x, mask) for self_attention in self.self_attentions]\n",
    "\n",
    "        # Concatenate the self attention outputs\n",
    "        # self_attention_outputs_concatenated dimensions are:\n",
    "        # (batch_size, sequence_length, number_of_heads * head_dimension)\n",
    "        concatenated_self_attention_outputs = torch.cat(self_attention_outputs, dim=2)\n",
    "\n",
    "        # Apply the output layer to the concatenated self attention outputs\n",
    "        # output dimensions are: (batch_size, sequence_length, embedding_dimension)\n",
    "        return self.output_layer(concatenated_self_attention_outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55b8e882",
   "metadata": {},
   "source": [
    "## Encoder feedforward layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "d3436165",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForward(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    Pytorch module for a feed forward layer.\n",
    "\n",
    "    A feed forward layer is a fully connected layer with a ReLU activation function in between.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, embedding_dimension, feed_forward_dimension):\n",
    "        super().__init__()\n",
    "        self.embedding_dimension = embedding_dimension\n",
    "        self.feed_forward_dimension = feed_forward_dimension\n",
    "        self.linear_1 = torch.nn.Linear(embedding_dimension, feed_forward_dimension)\n",
    "        self.linear_2 = torch.nn.Linear(feed_forward_dimension, embedding_dimension)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Compute the feed forward layer.\n",
    "        \"\"\"\n",
    "        return self.linear_2(torch.relu(self.linear_1(x)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4ec2c8d",
   "metadata": {},
   "source": [
    "## Decoder layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "52edabc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderLayer(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    Pytorch module for an encoder layer.\n",
    "\n",
    "    An encoder layer consists of a multi-headed self attention layer, a feed forward layer and dropout.\n",
    "\n",
    "    Input dimension is: (batch_size, sequence_length, embedding_dimension)\n",
    "    Output dimension is: (batch_size, sequence_length, embedding_dimension)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "            self,\n",
    "            embedding_dimension,\n",
    "            number_of_heads,\n",
    "            feed_forward_dimension,\n",
    "            dropout_rate\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.embedding_dimension = embedding_dimension\n",
    "        self.number_of_heads = number_of_heads\n",
    "        self.feed_forward_dimension = feed_forward_dimension\n",
    "        self.dropout_rate = dropout_rate\n",
    "\n",
    "        self.multi_headed_self_attention = MaskedMultiHeadedSelfAttention(embedding_dimension, number_of_heads)\n",
    "        self.feed_forward = FeedForward(embedding_dimension, feed_forward_dimension)\n",
    "        self.dropout = torch.nn.Dropout(dropout_rate)\n",
    "        self.layer_normalization_1 = torch.nn.LayerNorm(embedding_dimension)\n",
    "        self.layer_normalization_2 = torch.nn.LayerNorm(embedding_dimension)\n",
    "\n",
    "    def forward(self, x, mask):\n",
    "        \"\"\"\n",
    "        Compute the encoder layer.\n",
    "\n",
    "        x dimensions are: (batch_size, sequence_length, embedding_dimension)\n",
    "        mask dimensions are: (batch_size, sequence_length)\n",
    "        mask values are: 0 or 1. 0 means the token is masked, 1 means the token is not masked.\n",
    "        \"\"\"\n",
    "\n",
    "        # Layer normalization 1\n",
    "        normalized_x = self.layer_normalization_1(x)\n",
    "\n",
    "        # Multi headed self attention\n",
    "        attention_output = self.multi_headed_self_attention(normalized_x, mask)\n",
    "\n",
    "        # Residual output\n",
    "        residual_output = x + attention_output\n",
    "\n",
    "        # Layer normalization 2\n",
    "        normalized_residual_output = self.layer_normalization_2(residual_output)\n",
    "\n",
    "        # Feed forward\n",
    "        feed_forward_output = self.feed_forward(normalized_residual_output)\n",
    "\n",
    "        # Dropout\n",
    "        if self.training:\n",
    "            feed_forward_output = self.dropout(feed_forward_output)\n",
    "\n",
    "        # Residual output\n",
    "        return residual_output + feed_forward_output\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b164edd",
   "metadata": {},
   "source": [
    "## Decoder stack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "23e83b1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderStack(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    The decoder stack consists of multiple decoder layers in sequence.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "            self,\n",
    "            embedding_dimension,\n",
    "            number_of_layers,\n",
    "            number_of_heads,\n",
    "            feed_forward_dimension,\n",
    "            dropout_rate,\n",
    "            max_sequence_length\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.embedding_dimension = embedding_dimension\n",
    "        self.number_of_layers = number_of_layers\n",
    "        self.number_of_heads = number_of_heads\n",
    "        self.feed_forward_dimension = feed_forward_dimension\n",
    "        self.dropout_rate = dropout_rate\n",
    "        self.max_sequence_length = max_sequence_length\n",
    "\n",
    "        # Create the encoder layers\n",
    "        self.encoder_layers = torch.nn.ModuleList(\n",
    "            [DecoderLayer(embedding_dimension, number_of_heads, feed_forward_dimension, dropout_rate) for _ in\n",
    "             range(number_of_layers)])\n",
    "\n",
    "    def forward(self, x, mask):\n",
    "        decoder_outputs = x\n",
    "        for decoder_layer in self.encoder_layers:\n",
    "            decoder_outputs = decoder_layer(decoder_outputs, mask)\n",
    "\n",
    "        return decoder_outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6df32098",
   "metadata": {},
   "source": [
    "## Language Model Head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "b1dd30fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LMHead(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    Pytorch module for the language model head.\n",
    "    The language model head is a linear layer that maps the embedding dimension to the vocabulary size.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, embedding_dimension, number_of_tokens):\n",
    "        super().__init__()\n",
    "        self.embedding_dimension = embedding_dimension\n",
    "        self.number_of_tokens = number_of_tokens\n",
    "        self.linear = torch.nn.Linear(embedding_dimension, number_of_tokens)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Compute the language model head.\n",
    "\n",
    "        x dimensions are: (batch_size, sequence_length, embedding_dimension)\n",
    "        output dimensions are: (batch_size, sequence_length, number_of_tokens)\n",
    "        \"\"\"\n",
    "        # Compute the linear layer\n",
    "        # linear_output dimensions are: (batch_size, sequence_length, number_of_tokens)\n",
    "        linear_output = self.linear(x)\n",
    "\n",
    "        return linear_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "446d1b5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LanguageModel(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    Pytorch module for a language model.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "            self,\n",
    "            number_of_tokens,  # The number of tokens in the vocabulary\n",
    "            max_sequence_length=512,  # The maximum sequence length to use for attention\n",
    "            embedding_dimension=512,  # The dimension of the token embeddings\n",
    "            number_of_layers=6,  # The number of decoder layers to use\n",
    "            number_of_heads=4,  # The number of attention heads to use\n",
    "            feed_forward_dimension=None,  # The dimension of the feed forward layer\n",
    "            dropout_rate=0.1  # The dropout rate to use\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.number_of_tokens = number_of_tokens\n",
    "        self.max_sequence_length = max_sequence_length\n",
    "        self.embedding_dimension = embedding_dimension\n",
    "        self.number_of_layers = number_of_layers\n",
    "        self.number_of_heads = number_of_heads\n",
    "\n",
    "        if feed_forward_dimension is None:\n",
    "            # GPT-2 paper uses 4 * embedding_dimension for the feed forward dimension\n",
    "            self.feed_forward_dimension = embedding_dimension * 4\n",
    "        else:\n",
    "            self.feed_forward_dimension = feed_forward_dimension\n",
    "\n",
    "        self.dropout_rate = dropout_rate\n",
    "\n",
    "        # Create the token embedding layer\n",
    "        self.token_embedding = TokenEmbedding(embedding_dimension, number_of_tokens)\n",
    "\n",
    "        # Create the positional encoding layer\n",
    "        self.positional_encoding = PositionalEncoding(embedding_dimension, max_sequence_length)\n",
    "\n",
    "        # Create the normalization layer\n",
    "        self.layer_normalization = torch.nn.LayerNorm(embedding_dimension)\n",
    "\n",
    "        # Create the decoder stack\n",
    "        self.decoder = DecoderStack(\n",
    "            embedding_dimension=embedding_dimension,\n",
    "            number_of_layers=number_of_layers,\n",
    "            number_of_heads=number_of_heads,\n",
    "            feed_forward_dimension=self.feed_forward_dimension,\n",
    "            dropout_rate=dropout_rate,\n",
    "            max_sequence_length=max_sequence_length\n",
    "        )\n",
    "\n",
    "        # Create the language model head\n",
    "        self.lm_head = LMHead(embedding_dimension, number_of_tokens)\n",
    "\n",
    "    def forward(self, x, mask):\n",
    "        # Compute the token embeddings\n",
    "        # token_embeddings dimensions are: (batch_size, sequence_length, embedding_dimension)\n",
    "        token_embeddings = self.token_embedding(x)\n",
    "\n",
    "        # Compute the positional encoding\n",
    "        # positional_encoding dimensions are: (batch_size, sequence_length, embedding_dimension)\n",
    "        positional_encoding = self.positional_encoding(token_embeddings)\n",
    "\n",
    "        # Post embedding layer normalization\n",
    "        positional_encoding_normalized = self.layer_normalization(positional_encoding)\n",
    "\n",
    "        decoder_outputs = self.decoder(positional_encoding_normalized, mask)\n",
    "        lm_head_outputs = self.lm_head(decoder_outputs)\n",
    "\n",
    "        return lm_head_outputs\n",
    "\n",
    "    def save_checkpoint(self, path):\n",
    "        print(f'Saving checkpoint {path}')\n",
    "        torch.save({\n",
    "            'number_of_tokens': self.number_of_tokens,\n",
    "            'max_sequence_length': self.max_sequence_length,\n",
    "            'embedding_dimension': self.embedding_dimension,\n",
    "            'number_of_layers': self.number_of_layers,\n",
    "            'number_of_heads': self.number_of_heads,\n",
    "            'feed_forward_dimension': self.feed_forward_dimension,\n",
    "            'dropout_rate': self.dropout_rate,\n",
    "            'model_state_dict': self.state_dict()\n",
    "        }, path)\n",
    "\n",
    "    @staticmethod\n",
    "    def load_checkpoint(path) -> 'LanguageModel':\n",
    "        checkpoint = torch.load(path)\n",
    "        model = LanguageModel(\n",
    "            number_of_tokens=checkpoint['number_of_tokens'],\n",
    "            max_sequence_length=checkpoint['max_sequence_length'],\n",
    "            embedding_dimension=checkpoint['embedding_dimension'],\n",
    "            number_of_layers=checkpoint['number_of_layers'],\n",
    "            number_of_heads=checkpoint['number_of_heads'],\n",
    "            feed_forward_dimension=checkpoint['feed_forward_dimension'],\n",
    "            dropout_rate=checkpoint['dropout_rate']\n",
    "        )\n",
    "        model.load_state_dict(checkpoint['model_state_dict'])\n",
    "        return model.to(get_device())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "6a3a8c5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AutoregressiveWrapper(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    Pytorch module that wraps a GPT model and makes it autoregressive.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, gpt_model):\n",
    "        super().__init__()\n",
    "        self.model = gpt_model\n",
    "        self.max_sequence_length = self.model.max_sequence_length\n",
    "\n",
    "    def forward(self, x, mask):\n",
    "        \"\"\"\n",
    "        Autoregressive forward pass\n",
    "        \"\"\"\n",
    "        inp, target = x[:, :-1], x[:, 1:]\n",
    "        mask = mask[:, :-1]\n",
    "\n",
    "        output = self.model(inp, mask)\n",
    "        return output, target\n",
    "\n",
    "    def next_token_probabilities(self, x, mask, temperature=1.0):\n",
    "        \"\"\"\n",
    "        Calculate the token probabilities for the next token in the sequence.\n",
    "        \"\"\"\n",
    "        logits = self.model(x, mask)[:, -1]\n",
    "\n",
    "        # Apply temperature\n",
    "        if temperature != 1.0:\n",
    "            logits = logits / temperature\n",
    "\n",
    "        # Apply the softmax\n",
    "        probabilities = torch.softmax(logits, dim=-1)\n",
    "\n",
    "        return probabilities\n",
    "\n",
    "    def save_checkpoint(self, path):\n",
    "        self.model.save_checkpoint(path)\n",
    "\n",
    "    @staticmethod\n",
    "    def load_checkpoint(path) -> 'AutoregressiveWrapper':\n",
    "        model = LanguageModel.load_checkpoint(path)\n",
    "        return AutoregressiveWrapper(model).to(get_device())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "763056b6",
   "metadata": {},
   "source": [
    "## Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "24231aad",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tokenizer:\n",
    "\n",
    "    def __init__(self):\n",
    "        self.dictionary = {}\n",
    "        self.reverse_dictionary = {}\n",
    "\n",
    "        # Add the padding token\n",
    "        self.__add_to_dict('<pad>')\n",
    "\n",
    "        # Add characters and numbers to the dictionary\n",
    "        for i in range(10):\n",
    "            self.__add_to_dict(str(i))\n",
    "        for i in range(26):\n",
    "            self.__add_to_dict(chr(ord('a') + i))\n",
    "\n",
    "        # Add space and punctuation to the dictionary\n",
    "        self.__add_to_dict('.')\n",
    "        self.__add_to_dict(' ')\n",
    "\n",
    "    def __add_to_dict(self, character):\n",
    "        if character not in self.dictionary:\n",
    "            self.dictionary[character] = len(self.dictionary)\n",
    "            self.reverse_dictionary[self.dictionary[character]] = character\n",
    "\n",
    "    def tokenize(self, text):\n",
    "        return [self.dictionary[c] for c in text]\n",
    "\n",
    "    def character_to_token(self, character):\n",
    "        return self.dictionary[character]\n",
    "\n",
    "    def token_to_character(self, token):\n",
    "        return self.reverse_dictionary[token]\n",
    "\n",
    "    def size(self):\n",
    "        return len(self.dictionary)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "653b201a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_device():\n",
    "    return torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "b8ad6ff7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Trainer:\n",
    "\n",
    "    def __init__(self, model, tokenizer: Tokenizer, optimizer=None):\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "        if optimizer is None:\n",
    "            self.optimizer = torch.optim.Adam(model.parameters(), lr=0.0001)\n",
    "        else:\n",
    "            self.optimizer = optimizer\n",
    "        self.tokenizer = tokenizer\n",
    "        self.loss_function = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "    def train(self, data: List[str], epochs, batch_size):\n",
    "        loss_per_epoch = []\n",
    "        for epoch in range(epochs):\n",
    "            losses = []\n",
    "\n",
    "            # Shuffle the sequences\n",
    "            random.shuffle(data)\n",
    "\n",
    "            # Create batches of sequences and their respective mask.\n",
    "            batches = []\n",
    "            for i in range(0, len(data), batch_size):\n",
    "                sequence_tensor = torch.tensor(data[i: i + batch_size], dtype=torch.long)\n",
    "\n",
    "                # Create the mask tensor for the batch, where 1 means the token is not a padding token\n",
    "                mask_tensor = torch.ones_like(sequence_tensor)\n",
    "                mask_tensor[sequence_tensor == self.tokenizer.character_to_token('<pad>')] = 0\n",
    "\n",
    "                batches.append((sequence_tensor, mask_tensor))\n",
    "\n",
    "            # Train the model on each batch\n",
    "            for batch in batches:\n",
    "                self.model.train()\n",
    "\n",
    "                # Create the input and mask tensors\n",
    "                input_tensor = torch.zeros((batch_size, self.model.max_sequence_length + 1), dtype=torch.long)\n",
    "                mask_tensor = torch.zeros((batch_size, self.model.max_sequence_length + 1), dtype=torch.long)\n",
    "\n",
    "                for i, input_entry in enumerate(batch[0]):\n",
    "                    input_tensor[i] = input_entry\n",
    "\n",
    "                for i, mask_entry in enumerate(batch[1]):\n",
    "                    mask_tensor[i] = mask_entry\n",
    "\n",
    "                # Compute the model output\n",
    "                model_output, target = self.model.forward(\n",
    "                    x=input_tensor.to(get_device()),\n",
    "                    mask=mask_tensor.to(get_device())\n",
    "                )\n",
    "\n",
    "                # Compute the losses\n",
    "                # The loss is computed on the model output and the target\n",
    "                loss = self.loss_function(model_output.transpose(1, 2), target)\n",
    "                # loss = self.loss_function(model_output[:, -1, :], target[:, -1])\n",
    "\n",
    "                # Backpropagate the loss.\n",
    "                loss.backward()\n",
    "\n",
    "                # Clip the gradients. This is used to prevent exploding gradients.\n",
    "                torch.nn.utils.clip_grad_norm_(self.model.parameters(), 0.5)\n",
    "\n",
    "                # Update the model parameters. This is done by taking a step in the direction of the gradient.\n",
    "                self.optimizer.step()\n",
    "\n",
    "                # Reset the gradients. This is done so that the gradients from the previous batch\n",
    "                # are not used in the next step.\n",
    "                self.optimizer.zero_grad()\n",
    "\n",
    "                # Append the loss to the list of losses, so that the average loss can be computed for this epoch.\n",
    "                losses.append(loss.item())\n",
    "\n",
    "            # Print the loss\n",
    "            epoch_loss = np.average(losses)\n",
    "            loss_per_epoch.append(epoch_loss)\n",
    "            print('Epoch:', epoch, 'Loss:', epoch_loss)\n",
    "\n",
    "        return loss_per_epoch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "41b08e98",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator:\n",
    "\n",
    "    def __init__(\n",
    "            self,\n",
    "            model,\n",
    "            tokenizer):\n",
    "        self.model = model\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "    def generate(\n",
    "            self,\n",
    "            max_tokens_to_generate: int,\n",
    "            prompt: str = None,\n",
    "            temperature: float = 1.0,\n",
    "            eos_token: int = None,\n",
    "            padding_token: int = 0):\n",
    "\n",
    "        self.model.eval()\n",
    "\n",
    "        if prompt is None:\n",
    "            start_tokens = [self.tokenizer.character_to_token(padding_token)]\n",
    "        else:\n",
    "            start_tokens = self.tokenizer.tokenize(prompt)\n",
    "\n",
    "        input_tensor = torch.tensor(\n",
    "            pad_left(\n",
    "                sequence=start_tokens,\n",
    "                final_length=self.model.max_sequence_length + 1,\n",
    "                padding_token=padding_token\n",
    "            ),\n",
    "            dtype=torch.long\n",
    "        ).to(get_device())\n",
    "\n",
    "        num_dims = len(input_tensor.shape)\n",
    "\n",
    "        if num_dims == 1:\n",
    "            input_tensor = input_tensor[None, :]\n",
    "\n",
    "        out = input_tensor\n",
    "        for _ in range(max_tokens_to_generate):\n",
    "\n",
    "            x = out[:, -self.model.max_sequence_length:]\n",
    "\n",
    "            mask = torch.ones_like(x)\n",
    "            mask[x == padding_token] = 0\n",
    "\n",
    "            # Compute the next token probabilities\n",
    "            next_token_probabilities = self.model.next_token_probabilities(\n",
    "                x=x,\n",
    "                temperature=temperature,\n",
    "                mask=mask\n",
    "            )\n",
    "\n",
    "            # Sample the next token from the probability distribution\n",
    "            next_token = torch.multinomial(next_token_probabilities, num_samples=1)\n",
    "\n",
    "            # Append the next token to the output\n",
    "            out = torch.cat([out, next_token], dim=1)\n",
    "\n",
    "            # If the end of sequence token is reached, stop generating tokens\n",
    "            if eos_token is not None and next_token == eos_token:\n",
    "                break\n",
    "\n",
    "        generated_tokens = out[0].tolist()\n",
    "        return ''.join([self.tokenizer.token_to_character(token) for token in generated_tokens])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "67ea87d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_training_sequences(max_sequence_length, tokenized_training_data):\n",
    "    # Create sequences of length max_sequence_length + 1\n",
    "    # The last token of each sequence is the target token\n",
    "    sequences = []\n",
    "    for i in range(0, len(tokenized_training_data) - max_sequence_length - 1):\n",
    "        sequences.append(tokenized_training_data[i: i + max_sequence_length + 1])\n",
    "    return sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "9b980df6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_and_pad_training_data(max_sequence_length, tokenizer, training_data):\n",
    "    # Tokenize the training data\n",
    "    tokenized_training_data = tokenizer.tokenize(training_data)\n",
    "    for _ in range(max_sequence_length):\n",
    "        # Prepend padding tokens\n",
    "        tokenized_training_data.insert(0, tokenizer.character_to_token('<pad>'))\n",
    "    return tokenized_training_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "ad2d4ada",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Runner(torch.nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def run(self):\n",
    "        # Create the tokenizer\n",
    "        tokenizer = Tokenizer()\n",
    "\n",
    "        embedding_dimension = 256\n",
    "        max_sequence_length = 20\n",
    "        number_of_tokens = tokenizer.size()\n",
    "\n",
    "        # Create the model\n",
    "        model = AutoregressiveWrapper(LanguageModel(\n",
    "            embedding_dimension=embedding_dimension,\n",
    "            number_of_tokens=number_of_tokens,\n",
    "            number_of_heads=4,\n",
    "            number_of_layers=3,\n",
    "            dropout_rate=0.1,\n",
    "            max_sequence_length=max_sequence_length\n",
    "        )).to(get_device())\n",
    "\n",
    "        # Create the training data\n",
    "        training_data = '. '.join([\n",
    "            'cats rule the world',\n",
    "            'dogs are the best',\n",
    "            'elephants have long trunks',\n",
    "            'monkeys like bananas',\n",
    "            'pandas eat bamboo',\n",
    "            'tigers are dangerous',\n",
    "            'zebras have stripes',\n",
    "            'lions are the kings of the savannah',\n",
    "            'giraffes have long necks',\n",
    "            'hippos are big and scary',\n",
    "            'rhinos have horns',\n",
    "            'cat and dog play togather',\n",
    "            'penguins live in the arctic',\n",
    "            'polar bears are white',\n",
    "            'my cat is playable'\n",
    "        ])\n",
    "\n",
    "        tokenized_and_padded_training_data = tokenize_and_pad_training_data(max_sequence_length, tokenizer, training_data)\n",
    "        sequences = create_training_sequences(max_sequence_length, tokenized_and_padded_training_data)\n",
    "\n",
    "        # Train the model\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=0.0001)\n",
    "        trainer = Trainer(model, tokenizer, optimizer)\n",
    "        loss_per_epoch = trainer.train(sequences, epochs=200, batch_size=16)\n",
    "\n",
    "        # Plot the loss per epoch in log scale\n",
    "        plt.plot(loss_per_epoch)\n",
    "        plt.yscale('log')\n",
    "        plt.ylabel('Loss')\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.show()\n",
    "\n",
    "        model.save_checkpoint('./trained_model')\n",
    "\n",
    "\n",
    "        print(\"input a prompt! like cat\")\n",
    "\n",
    "        prompt = input()\n",
    "\n",
    "        while(prompt != \"exit\"):\n",
    "            # Generate text\n",
    "            prompt = input()\n",
    "            max_tokens_to_generate = 100\n",
    "            generator = Generator(model, tokenizer)\n",
    "            generated_text = generator.generate(\n",
    "                max_tokens_to_generate=max_tokens_to_generate,\n",
    "                prompt= prompt,\n",
    "                padding_token=tokenizer.character_to_token('<pad>')\n",
    "            )\n",
    "            print(generated_text.replace('<pad>', ''))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "841e87bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def pad_left(sequence, final_length, padding_token):\n",
    "    return [padding_token] * (final_length - len(sequence)) + sequence\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52c79652",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 Loss: 2.6013866429743557\n",
      "Epoch: 1 Loss: 1.904720998328665\n",
      "Epoch: 2 Loss: 1.6683686323787854\n",
      "Epoch: 3 Loss: 1.499748539665471\n",
      "Epoch: 4 Loss: 1.339171235976012\n",
      "Epoch: 5 Loss: 1.2034934681394827\n",
      "Epoch: 6 Loss: 1.0664591374604597\n",
      "Epoch: 7 Loss: 0.933285187120023\n",
      "Epoch: 8 Loss: 0.8250175319288088\n",
      "Epoch: 9 Loss: 0.7234629224175992\n",
      "Epoch: 10 Loss: 0.6282318215007368\n",
      "Epoch: 11 Loss: 0.5335028767585754\n",
      "Epoch: 12 Loss: 0.44931302057660144\n",
      "Epoch: 13 Loss: 0.3734059269013612\n",
      "Epoch: 14 Loss: 0.3144831657409668\n",
      "Epoch: 15 Loss: 0.26319391960683075\n",
      "Epoch: 16 Loss: 0.2227418353052243\n",
      "Epoch: 17 Loss: 0.1916897015079208\n",
      "Epoch: 18 Loss: 0.16420434204780537\n",
      "Epoch: 19 Loss: 0.14220587779646335\n",
      "Epoch: 20 Loss: 0.12272419812886612\n",
      "Epoch: 21 Loss: 0.10821946904711101\n",
      "Epoch: 22 Loss: 0.09585392434635888\n",
      "Epoch: 23 Loss: 0.08533143875715525\n",
      "Epoch: 24 Loss: 0.07046926086363585\n",
      "Epoch: 25 Loss: 0.06435630928077128\n",
      "Epoch: 26 Loss: 0.05839804600438346\n",
      "Epoch: 27 Loss: 0.05093781795838605\n",
      "Epoch: 28 Loss: 0.04684908752856047\n",
      "Epoch: 29 Loss: 0.044973189011216164\n",
      "Epoch: 30 Loss: 0.04026402443971323\n",
      "Epoch: 31 Loss: 0.03541919155775205\n",
      "Epoch: 32 Loss: 0.03771439421436061\n",
      "Epoch: 33 Loss: 0.03403220196133074\n",
      "Epoch: 34 Loss: 0.029344617328404085\n",
      "Epoch: 35 Loss: 0.026534136222756428\n",
      "Epoch: 36 Loss: 0.024233447954706524\n",
      "Epoch: 37 Loss: 0.03171394947592331\n",
      "Epoch: 38 Loss: 0.030264847021064033\n",
      "Epoch: 39 Loss: 0.03593674174550435\n",
      "Epoch: 40 Loss: 0.024692544318816585\n",
      "Epoch: 41 Loss: 0.02157217810821274\n",
      "Epoch: 42 Loss: 0.023168185417054465\n",
      "Epoch: 43 Loss: 0.021397701990993126\n",
      "Epoch: 44 Loss: 0.021582375148959134\n",
      "Epoch: 45 Loss: 0.02004128728952745\n",
      "Epoch: 46 Loss: 0.026377048596496814\n",
      "Epoch: 47 Loss: 0.014093057394189678\n",
      "Epoch: 48 Loss: 0.01509203099767151\n",
      "Epoch: 49 Loss: 0.016886300285873207\n",
      "Epoch: 50 Loss: 0.019470885416249865\n",
      "Epoch: 51 Loss: 0.011574388309584363\n",
      "Epoch: 52 Loss: 0.012288316598405008\n",
      "Epoch: 53 Loss: 0.012679799328274701\n",
      "Epoch: 54 Loss: 0.01285901852964383\n",
      "Epoch: 55 Loss: 0.014423471025150755\n",
      "Epoch: 56 Loss: 0.008965244425622665\n",
      "Epoch: 57 Loss: 0.010874842470714255\n",
      "Epoch: 58 Loss: 0.01365615080272698\n",
      "Epoch: 59 Loss: 0.012312532169744372\n",
      "Epoch: 60 Loss: 0.011071420715028502\n",
      "Epoch: 61 Loss: 0.010115054107028182\n",
      "Epoch: 62 Loss: 0.011341728366223042\n",
      "Epoch: 63 Loss: 0.009077499974923938\n",
      "Epoch: 64 Loss: 0.014309482962783912\n",
      "Epoch: 65 Loss: 0.009909800992792716\n",
      "Epoch: 66 Loss: 0.007020174971093302\n",
      "Epoch: 67 Loss: 0.010866523877231648\n",
      "Epoch: 68 Loss: 0.007295460703418306\n",
      "Epoch: 69 Loss: 0.006821984673202362\n",
      "Epoch: 70 Loss: 0.006702388996136901\n",
      "Epoch: 71 Loss: 0.008721398857786604\n",
      "Epoch: 72 Loss: 0.009486140237878199\n",
      "Epoch: 73 Loss: 0.007954743761650247\n",
      "Epoch: 74 Loss: 0.008714352385140955\n",
      "Epoch: 75 Loss: 0.006048313602703907\n",
      "Epoch: 76 Loss: 0.005608099163509905\n",
      "Epoch: 77 Loss: 0.004597862440910514\n",
      "Epoch: 78 Loss: 0.004481017979307343\n",
      "Epoch: 79 Loss: 0.006721852415585486\n",
      "Epoch: 80 Loss: 0.004559619094083167\n",
      "Epoch: 81 Loss: 0.0034785108038467233\n",
      "Epoch: 82 Loss: 0.006464167832113479\n",
      "Epoch: 83 Loss: 0.006138926516185798\n",
      "Epoch: 84 Loss: 0.011375409887556958\n",
      "Epoch: 85 Loss: 0.006449359547569538\n",
      "Epoch: 86 Loss: 0.007095571951535733\n",
      "Epoch: 87 Loss: 0.008463913195199617\n",
      "Epoch: 88 Loss: 0.0041297622741220275\n",
      "Epoch: 89 Loss: 0.00556029423656266\n",
      "Epoch: 90 Loss: 0.007631368522593023\n",
      "Epoch: 91 Loss: 0.0035004756367846353\n",
      "Epoch: 92 Loss: 0.004907486337722968\n",
      "Epoch: 93 Loss: 0.00530298481411908\n",
      "Epoch: 94 Loss: 0.0029519533234871114\n",
      "Epoch: 95 Loss: 0.007337635713275117\n",
      "Epoch: 96 Loss: 0.00213861408638363\n",
      "Epoch: 97 Loss: 0.00199878524037321\n",
      "Epoch: 98 Loss: 0.0024415086075390245\n",
      "Epoch: 99 Loss: 0.0028358990890885016\n",
      "Epoch: 100 Loss: 0.0022685802136750326\n",
      "Epoch: 101 Loss: 0.0017320508031053064\n",
      "Epoch: 102 Loss: 0.005656044193766202\n",
      "Epoch: 103 Loss: 0.002837802540971493\n",
      "Epoch: 104 Loss: 0.003038320278510978\n",
      "Epoch: 105 Loss: 0.006257394870930432\n",
      "Epoch: 106 Loss: 0.004081514123042681\n",
      "Epoch: 107 Loss: 0.004282912036970905\n",
      "Epoch: 108 Loss: 0.006175516108708942\n",
      "Epoch: 109 Loss: 0.004645033375076626\n",
      "Epoch: 110 Loss: 0.003080998492963693\n",
      "Epoch: 111 Loss: 0.0035304093819476016\n",
      "Epoch: 112 Loss: 0.009261852015143908\n",
      "Epoch: 113 Loss: 0.012589079915019482\n",
      "Epoch: 114 Loss: 0.011413198298491214\n",
      "Epoch: 115 Loss: 0.010914429009932539\n",
      "Epoch: 116 Loss: 0.006354412182902351\n",
      "Epoch: 117 Loss: 0.0074462606166691885\n",
      "Epoch: 118 Loss: 0.011939358834237994\n",
      "Epoch: 119 Loss: 0.011506018056736692\n",
      "Epoch: 120 Loss: 0.017194263034743137\n",
      "Epoch: 121 Loss: 0.007340847342477545\n",
      "Epoch: 122 Loss: 0.006903819927095395\n",
      "Epoch: 123 Loss: 0.005109502397396643\n",
      "Epoch: 124 Loss: 0.0026674631352880565\n",
      "Epoch: 125 Loss: 0.006592804983602432\n",
      "Epoch: 126 Loss: 0.003107433852678894\n",
      "Epoch: 127 Loss: 0.005026074929881598\n",
      "Epoch: 128 Loss: 0.00763941756905178\n",
      "Epoch: 129 Loss: 0.005444333183771485\n",
      "Epoch: 130 Loss: 0.003984952987040109\n",
      "Epoch: 131 Loss: 0.0021986264093910627\n",
      "Epoch: 132 Loss: 0.003578940698745377\n",
      "Epoch: 133 Loss: 0.01006759086601011\n",
      "Epoch: 134 Loss: 0.007015328947644499\n",
      "Epoch: 135 Loss: 0.003419871678014043\n",
      "Epoch: 136 Loss: 0.005664430546027649\n",
      "Epoch: 137 Loss: 0.0032679768837229144\n",
      "Epoch: 138 Loss: 0.003502281617408659\n",
      "Epoch: 139 Loss: 0.003732677025254816\n",
      "Epoch: 140 Loss: 0.0014977043471805266\n",
      "Epoch: 141 Loss: 0.001411171799902703\n",
      "Epoch: 142 Loss: 0.00118779465613087\n",
      "Epoch: 143 Loss: 0.0012299577701133271\n",
      "Epoch: 144 Loss: 0.002009338750212919\n",
      "Epoch: 145 Loss: 0.002773698561055505\n",
      "Epoch: 146 Loss: 0.0030503963985804307\n",
      "Epoch: 147 Loss: 0.0025567221028208937\n",
      "Epoch: 148 Loss: 0.00424885786165048\n",
      "Epoch: 149 Loss: 0.002772659971884897\n",
      "Epoch: 150 Loss: 0.0031815845811607965\n",
      "Epoch: 151 Loss: 0.0027758221182486286\n",
      "Epoch: 152 Loss: 0.002676894872148177\n",
      "Epoch: 153 Loss: 0.0023485817396051616\n",
      "Epoch: 154 Loss: 0.004959224886514004\n",
      "Epoch: 155 Loss: 0.012448713183403015\n",
      "Epoch: 156 Loss: 0.013296529832903458\n",
      "Epoch: 157 Loss: 0.013825716035764503\n",
      "Epoch: 158 Loss: 0.01195280537459716\n",
      "Epoch: 159 Loss: 0.005277595103151449\n",
      "Epoch: 160 Loss: 0.011310846443571474\n",
      "Epoch: 161 Loss: 0.013477425237753145\n",
      "Epoch: 162 Loss: 0.01044423225507869\n",
      "Epoch: 163 Loss: 0.005859330691316206\n",
      "Epoch: 164 Loss: 0.006896537037075335\n",
      "Epoch: 165 Loss: 0.0023832361380893576\n",
      "Epoch: 166 Loss: 0.00167899393430992\n",
      "Epoch: 167 Loss: 0.002025215991560127\n",
      "Epoch: 168 Loss: 0.003351440143887885\n",
      "Epoch: 169 Loss: 0.0034416609269101173\n",
      "Epoch: 170 Loss: 0.004859441175105293\n",
      "Epoch: 171 Loss: 0.001976548053789884\n",
      "Epoch: 172 Loss: 0.003077778168042879\n",
      "Epoch: 173 Loss: 0.0017452046044838978\n",
      "Epoch: 174 Loss: 0.0025601308880141005\n",
      "Epoch: 175 Loss: 0.00390862330601996\n",
      "Epoch: 176 Loss: 0.003072247184362546\n",
      "Epoch: 177 Loss: 0.0011515191255729524\n",
      "Epoch: 178 Loss: 0.001509607318439521\n",
      "Epoch: 179 Loss: 0.0026689348410202556\n",
      "Epoch: 180 Loss: 0.0014855990318566278\n",
      "Epoch: 181 Loss: 0.004427521407626994\n",
      "Epoch: 182 Loss: 0.005377222979681708\n",
      "Epoch: 183 Loss: 0.008638384558371794\n",
      "Epoch: 184 Loss: 0.0043443320458248745\n",
      "Epoch: 185 Loss: 0.0021964872503303923\n",
      "Epoch: 186 Loss: 0.002578042029990288\n",
      "Epoch: 187 Loss: 0.005600621958602342\n",
      "Epoch: 188 Loss: 0.007144751184134056\n",
      "Epoch: 189 Loss: 0.0045182450868077985\n",
      "Epoch: 190 Loss: 0.0017293886130969242\n",
      "Epoch: 191 Loss: 0.0014169049843432099\n",
      "Epoch: 192 Loss: 0.001143148273948337\n",
      "Epoch: 193 Loss: 0.0009734974172834368\n",
      "Epoch: 194 Loss: 0.0010839033523592186\n",
      "Epoch: 195 Loss: 0.0011961804590575175\n",
      "Epoch: 196 Loss: 0.0018290777531902954\n",
      "Epoch: 197 Loss: 0.002490724510963723\n",
      "Epoch: 198 Loss: 0.0013055590486898006\n",
      "Epoch: 199 Loss: 0.0013055306473055996\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving checkpoint ./trained_model\n",
      "input a prompt! like cat\n",
      "the cat\n",
      "the cat\n",
      "the cats at ayayayayayayayayayayayayayayayayayayayayayayayayayabababadadadadadadadadadadadadadadadadadadada\n",
      "dogs and cats\n",
      "dogs and cats at ay ayayayayayayayayayayayayayayayayayayayayayayayayayayayayayayayayayayayayayayayayayayayayayaya\n",
      "human\n",
      "humandandandandandandandandandandandandandandandandandandandandandandogererererers have long ng necks. hi\n",
      "lion\n",
      "lions are the kings of the savannah. giraffes have long necks. hippos are big and scary. rhinos have hor\n",
      "cat and lion\n",
      "cat and lions and dog the best. elephants have long trunks. monkeys like bananas. pandas eat bamboo. tigers are \n",
      "human and dog\n",
      "human and dog platog therus therus rins live in the arctic. polar bears are white. my cat is playablablablamblamb\n"
     ]
    }
   ],
   "source": [
    "\n",
    "Runner().run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b8ff02f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed354775",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tf-gpu] *",
   "language": "python",
   "name": "conda-env-tf-gpu-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
