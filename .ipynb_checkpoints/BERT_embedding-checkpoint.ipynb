{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2c05e65a",
   "metadata": {},
   "source": [
    "## Vectorization and Embedding \n",
    "- The word “tokenizer” was split into the known words “token” and “##izer,” where “##” indicates that the token should be attached to the previous one.\n",
    "\n",
    "- By using subword tokenization, we can take advantage of the benefits of word tokenization while keeping the vocabulary size reasonable. For example, the `bert-case-uncased` tokenizer used in the example above has a vocabulary size of only 30,522 tokens.\n",
    "\n",
    "- Bert: https://arxiv.org/abs/1810.04805 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63e53ab4",
   "metadata": {},
   "source": [
    "## Tokenization\n",
    "\n",
    "### Using pre-trained bert-base-uncased tokenizer to tokenize a sample sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "40e56c53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['this', 'is', 'an', 'example', 'of', 'the', 'bert', 'token', '##izer']\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "tokens = tokenizer.tokenize(\"This is an example of the bert tokenizer\")\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45b10a89",
   "metadata": {},
   "source": [
    "## token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "06490d37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2023, 2003, 2019, 2742, 1997, 1996, 14324, 19204, 17629]\n"
     ]
    }
   ],
   "source": [
    "token_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "print(token_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23b59b6a",
   "metadata": {},
   "source": [
    "## Token Embeddings\n",
    "\n",
    "- The returned word vector has a size of 768 dimensions, the same as the BERT model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f33e116a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "model.safetensors: 100%|████████████████████████████████████████████████| 440M/440M [01:11<00:00, 6.15MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 768])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import BertModel\n",
    "\n",
    "model = BertModel.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "# get the embedding vector for the word \"example\"\n",
    "example_token_id = tokenizer.convert_tokens_to_ids([\"example\"])[0]\n",
    "example_embedding = model.embeddings.word_embeddings(torch.tensor([example_token_id]))\n",
    "\n",
    "print(example_embedding.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97cbef3e",
   "metadata": {},
   "source": [
    "## Vector comparision\n",
    "\n",
    "- We can use these vectors to compare their similarities by using PyTorch’s cosine similarity function.\n",
    "\n",
    "- Cosine similarity is a way to measure how similar two things are. It’s often used in natural language processing to compare the content of two texts.\n",
    "\n",
    "- To calculate the cosine similarity, we look at the angle between two vectors. If the vectors point in the same direction, they are more similar, and if they point in opposite directions, they are less similar.\n",
    "\n",
    "- The result is a number between -1 and 1, where 1 means the vectors are identical and -1 means they are completely different."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "77d3abb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.6469, grad_fn=<SelectBackward0>)\n"
     ]
    }
   ],
   "source": [
    "king_token_id = tokenizer.convert_tokens_to_ids([\"king\"])[0]\n",
    "king_embedding = model.embeddings.word_embeddings(torch.tensor([king_token_id]))\n",
    "\n",
    "queen_token_id = tokenizer.convert_tokens_to_ids([\"queen\"])[0]\n",
    "queen_embedding = model.embeddings.word_embeddings(torch.tensor([queen_token_id]))\n",
    "\n",
    "cos = torch.nn.CosineSimilarity(dim=1)\n",
    "similarity = cos(king_embedding, queen_embedding)\n",
    "print(similarity[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "69e11e80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.2392, grad_fn=<SelectBackward0>)\n"
     ]
    }
   ],
   "source": [
    "similarity = cos(example_embedding, queen_embedding)\n",
    "print(similarity[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f469dab",
   "metadata": {},
   "source": [
    "- The queen and example vectors have a similarity of 0.2392. This means that the king and queen vectors are more similar in our vector space than the example and queen vectors. This show that our model successfully learned the “meaning” of these words and can differentiate between them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e329f4b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tf-gpu] *",
   "language": "python",
   "name": "conda-env-tf-gpu-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
